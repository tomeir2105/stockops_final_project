######################################
# Created by : Meir
# Purpose : Prepare SSH on controller, authorize controller key on nodes, write /etc/hosts, and configure NFS — without duplicates
# Date : 2025-10-30
# Version : 1
######################################
---
###############################################################################
# STAGE 0 — SSH | Prepare controller (keys, known_hosts)
###############################################################################
- name: Stage 0 — SSH | Prepare controller (keys, known_hosts)
  hosts: k3srouter_host
  gather_facts: true
  become: false
  vars_files:
    - "{{ playbook_dir }}/../vars.yml"

  pre_tasks:
    - name: Default controller SSH paths when undefined
      set_fact:
        CONTROLLER_KEY_PATH: "{{ CONTROLLER_KEY_PATH | default(ansible_env.HOME + '/.ssh/id_ed25519') }}"
        CONTROLLER_KNOWN_HOSTS_PATH: "{{ CONTROLLER_KNOWN_HOSTS_PATH | default(ansible_env.HOME + '/.ssh/known_hosts') }}"

    - name: Derive controller ssh dir
      set_fact:
        CONTROLLER_SSH_DIR: "{{ CONTROLLER_KEY_PATH | regex_replace('/[^/]+$','') }}"

  tasks:
    - name: Ensure openssh-client is installed on controller
      become: true
      package:
        name: openssh-client
        state: present

    - name: Ensure controller .ssh directory exists
      file:
        path: "{{ CONTROLLER_SSH_DIR }}"
        state: directory
        mode: "0700"

    - name: Ensure known_hosts file exists with strict perms
      file:
        path: "{{ CONTROLLER_KNOWN_HOSTS_PATH }}"
        state: touch
        mode: "0600"

    - name: Create ed25519 keypair if missing
      community.crypto.openssh_keypair:
        path: "{{ CONTROLLER_KEY_PATH }}"
        type: ed25519
        state: present
        mode: "0600"

    - name: Build list of hosts to scan (from inventory group ssh_scan_hosts)
      set_fact:
        _ssh_scan_hosts: "{{ groups['ssh_scan_hosts'] | default([]) }}"

    - name: ssh-keyscan (hashed) for ed25519, ecdsa, rsa
      command: "ssh-keyscan -T 5 -H -t ed25519,ecdsa,rsa {{ item }}"
      register: _scan_results
      changed_when: false
      failed_when: false
      loop: "{{ _ssh_scan_hosts }}"
      loop_control:
        label: "{{ item }}"

    - name: Add scanned host keys to known_hosts (idempotent)
      when: (item.stdout | default('') | trim) != ''
      known_hosts:
        path: "{{ CONTROLLER_KNOWN_HOSTS_PATH }}"
        name: "{{ item.item }}"
        key: "{{ item.stdout }}"
        state: present
      loop: "{{ _scan_results.results }}"
      loop_control:
        label: "{{ item.item }}"

###############################################################################
# STAGE 1 — SSH | Authorize controller public key on nodes
###############################################################################
- name: Stage 1 — SSH | Install controller public key on nodes
  hosts: k3s_server:k3s_workers
  gather_facts: false
  become: true
  vars_files:
    - "{{ playbook_dir }}/../vars.yml"
  vars:
    # Ensure Ansible uses the same key you use manually
    ansible_ssh_private_key_file: "{{ CONTROLLER_KEY_PATH }}"

  pre_tasks:
    # Derive target user from inventory
    - name: Derive target user from inventory (ansible_user)
      set_fact:
        TARGET_USER: "{{ hostvars[inventory_hostname].ansible_user | default('user') }}"

    # --- New: connectivity pre-check with friendly fallback ---
    - name: Precheck SSH connectivity to {{ inventory_hostname }}
      block:
        - name: Wait for SSH connectivity (quick check)
          wait_for_connection:
            timeout: 5
      rescue:
        - name: Hint — bootstrap keys with add-keys.sh from its folder, then re-run playbook
          debug:
            msg: >
              SSH to {{ inventory_hostname }} ({{ hostvars[inventory_hostname].ansible_host | default(inventory_hostname) }}) failed.
              To bootstrap keys once, run:
              cd /home/user/stockops_final_project/rpi-wifi-router &&
              ./add-keys.sh -u {{ TARGET_USER }}
              -H {{ hostvars[inventory_hostname].ansible_host | default(inventory_hostname) }}
        - meta: end_host

    # Ensure controller pubkey exists/slurp once on controller
    - name: Check controller public key exists (on controller)
      stat:
        path: "{{ CONTROLLER_KEY_PATH }}.pub"
      register: _pub
      delegate_to: "{{ groups['k3srouter_host'][0] }}"
      run_once: true

    - name: Fail if controller public key missing
      when: not _pub.stat.exists
      fail:
        msg: "Controller public key {{ CONTROLLER_KEY_PATH }}.pub not found."

    - name: Read controller public key once (on controller)
      slurp:
        src: "{{ CONTROLLER_KEY_PATH }}.pub"
      register: _pubkey
      delegate_to: "{{ groups['k3srouter_host'][0] }}"
      run_once: true

  tasks:
    # Try to authorize the key; on any failure, print the add-keys hint for this host
    - name: Authorize controller public key (idempotent, no duplicates)
      block:
        - ansible.posix.authorized_key:
            user: "{{ TARGET_USER }}"
            key: "{{ _pubkey.content | b64decode }}"
            state: present
            manage_dir: true
      rescue:
        - name: Hint — bootstrap keys with add-keys.sh from its folder, then re-run playbook
          debug:
            msg: >
              Authorizing key on {{ inventory_hostname }} failed.
              From the controller, run:
              cd /home/user/stockops_final_project/rpi-wifi-router &&
              ./add-keys.sh -u {{ TARGET_USER }}
              -H {{ hostvars[inventory_hostname].ansible_host | default(inventory_hostname) }}
        - meta: end_host

###############################################################################
# STAGE 1B — SSH | Unified /etc/hosts block
###############################################################################
- name: Stage 1B — SSH | Write unified /etc/hosts block (router + servers + workers)
  hosts: k3srouter_host:k3s_server:k3s_workers
  gather_facts: false
  become: true
  vars_files:
    - "{{ playbook_dir }}/../vars.yml"

  pre_tasks:
    - name: Build ordered unique host list (router, server, workers)
      set_fact:
        _all_nodes: "{{ (['k3srouter_host','k3s_server','k3s_workers'] | map('extract', groups) | list) | flatten | unique }}"

  tasks:
    - name: Write managed /etc/hosts block exactly once
      blockinfile:
        path: /etc/hosts
        create: true
        marker: "# {mark} ANSIBLE MANAGED BLOCK (k3s cluster hosts)"
        block: |
          {% for h in _all_nodes %}
          {{ hostvars[h].ansible_host }} {{ h }}
          {% endfor %}

###############################################################################
# STAGE 2 — NFS | Configure NFS server (folder-only)
###############################################################################
- name: Stage 2 — NFS | Configure NFS server (folder-only)
  hosts: k3srouter_host
  gather_facts: true
  become: true
  vars_files:
    - "{{ playbook_dir }}/../vars.yml"

  vars:
    NFS_MOUNTPOINT: "{{ NFS_MOUNTPOINT }}"
    NFS_EXPORTS_FILE: "{{ NFS_EXPORTS_FILE }}"
    NFS_ALLOWED_CIDR: "{{ NFS_ALLOWED_CIDR }}"
    NFS_EXPORT_OPTS: "{{ NFS_EXPORT_OPTS }}"

  tasks:
    - name: Remove any fstab entry that mounts /mnt/k3s_storage from a device/label/uuid
      lineinfile:
        path: /etc/fstab
        state: absent
        regexp: '^[#\s]*(LABEL=|UUID=|/dev/[^ \t]+)\s+/mnt/k3s_storage\b'
      notify: Reload systemd

    - name: Stop/disable generated mount unit if present
      command: systemctl disable --now mnt-k3s_storage.mount
      register: _mnt_disable
      failed_when: false
      changed_when: "'Removed' in _mnt_disable.stdout or 'disabled' in _mnt_disable.stdout"

    - name: Install NFS packages
      package:
        name:
          - nfs-kernel-server
          - nfs-common
          - rpcbind
        state: present

    - name: Ensure base path exists (folder we will export)
      file:
        path: "{{ NFS_MOUNTPOINT }}"
        state: directory
        owner: "root"
        group: "root"
        mode: "0775"

    - name: Create app directories
      file:
        path: "{{ item.path }}"
        state: directory
        owner: "{{ item.owner }}"
        group: "{{ item.group }}"
        mode: "{{ item.mode }}"
      loop: "{{ APP_DIRS | default([]) }}"
    - name: Ensure exports.d directory exists
      file:
        path: "{{ NFS_EXPORTS_FILE | dirname }}"
        state: directory
        owner: "root"
        group: "root"
        mode: "0755"

    - name: Write a single authoritative exports.d file
      copy:
        dest: "{{ NFS_EXPORTS_FILE }}"
        mode: "0644"
        content: |
          # Managed by Ansible — DO NOT EDIT
          {{ NFS_MOUNTPOINT }} {{ NFS_ALLOWED_CIDR }}({{ NFS_EXPORT_OPTS }})

    - name: Reload exports
      command: exportfs -ra

    - name: Enable/start rpcbind
      service:
        name: rpcbind
        state: started
        enabled: true

    - name: Enable/start NFS
      service:
        name: nfs-kernel-server
        state: started
        enabled: true

    - name: VERIFY | base export exists
      command: exportfs -v
      register: _exports
      changed_when: false

    - name: VERIFY | assert base export is present
      assert:
        that:
          - NFS_MOUNTPOINT in _exports.stdout
          - (NFS_ALLOWED_CIDR.split('/'))[0] in _exports.stdout
        fail_msg: |
          Expected export for {{ NFS_MOUNTPOINT }} not found.
          Got:
          {{ _exports.stdout }}

  handlers:
    - name: Reload systemd
      command: systemctl daemon-reload

